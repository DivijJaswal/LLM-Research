{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivijJaswal/LLM-Research/blob/main/Test_Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbwzfrma7GAu",
        "outputId": "cf8e0845-1e10-4dbc-8694-aece9fabb89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLM-Research'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 24 (delta 12), reused 14 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (24/24), 9.64 KiB | 9.64 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DivijJaswal/LLM-Research.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.decomposition import TruncatedSVD\n"
      ],
      "metadata": {
        "id": "orif1WIM4VS6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shorten_text(text, chunk_size, method, summarizer = None):\n",
        "    \"\"\"\n",
        "    Shortens the text using the specified method.\n",
        "    :param text: The original text to shorten.\n",
        "    :param chunk_size: The size of each chunk if needed.\n",
        "    :param method: Method to shorten the text.\n",
        "    :return: Shortened text.\n",
        "    \"\"\"\n",
        "    if method == \"clipping\":\n",
        "        return text[:chunk_size]\n",
        "\n",
        "    elif method == \"iterative\":\n",
        "        chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "        summarized_chunks = [summarizer(chunk, min_length=50, max_length=100, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "        return \" \".join(summarized_chunks)\n",
        "\n",
        "    elif method == \"random_removal\":\n",
        "        words = text.split()\n",
        "        while len(words) > chunk_size:\n",
        "            index_to_remove = random.randint(0, len(words) - 1)\n",
        "            del words[index_to_remove]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    elif method == \"sentence_ranking\":\n",
        "        return tfidf_sentence_ranking(text, chunk_size)\n",
        "\n",
        "    elif method == \"sliding_window\":\n",
        "        return sliding_window(text, chunk_size)\n",
        "\n",
        "    elif method == \"entity_filtering\":\n",
        "        return entity_filtering(text, chunk_size)\n",
        "\n",
        "    elif method == \"summarize_summary\":\n",
        "        return summarize_summary(text, chunk_size)\n",
        "\n",
        "    elif method == \"lsa\":\n",
        "        return lsa_text_summarization(text, chunk_size)\n",
        "\n",
        "def tfidf_sentence_ranking(text, chunk_size):\n",
        "    sentences = text.split('. ')\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "    sentence_scores = np.sum(vectors, axis=1)\n",
        "\n",
        "    ranked_sentences = sorted(((score, i, s) for i, (score, s) in enumerate(zip(sentence_scores, sentences))), reverse=True)\n",
        "    selected_sentences = [s for _, _, s in ranked_sentences[:int(chunk_size / 20)]]\n",
        "    return '. '.join(selected_sentences)\n",
        "\n",
        "def sliding_window(text, chunk_size, overlap=100, summarizer = None):\n",
        "    words = text.split()\n",
        "    windows = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        window = words[i:i+chunk_size]\n",
        "        windows.append(\" \".join(window))\n",
        "\n",
        "    summarized_windows = [summarizer(w, min_length=50, max_length=100, do_sample=False)[0]['summary_text'] for w in windows]\n",
        "    return \" \".join(summarized_windows)\n",
        "\n",
        "def entity_filtering(text, chunk_size):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    important_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        if any(ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"DATE\"] for ent in sent.ents):\n",
        "            important_sentences.append(sent.text)\n",
        "        if len(important_sentences) >= chunk_size / 20:\n",
        "            break\n",
        "    return \" \".join(important_sentences)\n",
        "\n",
        "\n",
        "def summarize_summary(text, chunk_size, min_length=200, summarizer = None):\n",
        "    \"\"\"\n",
        "    Recursively summarize the text until it is under a desired length.\n",
        "    \"\"\"\n",
        "    summarized_text = text\n",
        "    iteration = 0\n",
        "    while len(summarized_text) > chunk_size:\n",
        "        print(f\"Iteration {iteration + 1}: Text too long. Summarizing again.\")\n",
        "        summarized_text = summarizer(summarized_text, num_beams=5, min_length=min_length, max_length=chunk_size, do_sample=False)[0]['summary_text']\n",
        "        iteration += 1\n",
        "    return summarized_text\n",
        "\n",
        "def lsa_text_summarization(text, chunk_size):\n",
        "    \"\"\"\n",
        "    Use Latent Semantic Analysis (LSA) to extract the most important concepts from the text and return the most relevant sentences.\n",
        "    \"\"\"\n",
        "    sentences = text.split('. ')\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    lsa_model = TruncatedSVD(n_components=1, n_iter=100)\n",
        "    lsa_model.fit(X)\n",
        "    lsa_scores = lsa_model.transform(X)\n",
        "\n",
        "    # Rank sentences by their relevance to the main topics\n",
        "    ranked_sentences = sorted(((lsa_scores[i, 0], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    # Select the top N sentences based on LSA scores\n",
        "    selected_sentences = [s for _, s in ranked_sentences[:int(chunk_size / 20)]]\n",
        "    return '. '.join(selected_sentences)"
      ],
      "metadata": {
        "id": "286dXQhH35O5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def summarize_text(text,num_beams = 5):\n",
        "\n",
        "    login(token = \"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\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "    summarizer = pipeline(\"summarization\", tokenizer=tokenizer,model=\"google/flan-t5-base\")\n",
        "\n",
        "    short_summary = summarizer(text ,num_beams, min_length = 50, max_length =100,do_sample=False)\n",
        "    medium_summary = summarizer(text ,num_beams, min_length = 100, max_length =150,do_sample=False)\n",
        "    large_summary = summarizer(text ,num_beams, min_length = 150, max_length =200,do_sample=False)\n",
        "\n",
        "    print(short_summary)\n",
        "    print(medium_summary)\n",
        "    print(large_summary)"
      ],
      "metadata": {
        "id": "tbZk_UxeN_R-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file=\"/content/LLM-Research/text1.txt\"\n",
        "print(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjZj5SyIPPG5",
        "outputId": "93a44aa8-60b0-4035-ed08-2042e2ce6144"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLM-Research/text1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "5Wu4aIFCPc-p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = shorten_text(text, chunk_size = 480, method = 'random_removal')"
      ],
      "metadata": {
        "id": "TmSNbXKoPhqI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qae2CbAmRaMM",
        "outputId": "2ed27232-91b8-4c5b-a263-0420a05300e5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_text(text, num_beams = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwdHODB0RbAt",
        "outputId": "34a0e5f3-6098-4ac1-bf1d-fb1438e5a605"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ignoring args : (5,)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
            "Ignoring args : (5,)\n",
            "Ignoring args : (5,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'ICC is the international federation responsible the governance the sport of cricket and Code of Conduct for Players and Player “Code of Conduct” is adopted and part of ICC’s continuing efforts maintain of cricket by (a) effective any participant from conducting themselves improperly on off field-of-play or manner that is contrary to the Spirit Cricket; and (b) robust proportionate procedure pursuant all of conduct can be dealt fairly, with certainty and an expeditious manner.'}]\n",
            "[{'summary_text': 'ICC CODE OF FOR PLAYERS AND PLAYER INTRODUCTION ICC is the international federation responsible the governance the sport of cricket and Code of Conduct for Players and Player is adopted and part of ICC’s continuing efforts maintain of cricket by (a) effective any participant from conducting themselves improperly on off field-of-play or manner that is contrary to the Spirit Cricket; and (b) robust proportionate procedure pursuant all of conduct can be dealt fairly, with certainty and an expeditious manner.'}]\n",
            "[{'summary_text': 'ICC CODE OF FOR PLAYERS AND PLAYER INTRODUCTION ICC is the international federation responsible the governance the sport of cricket and Code of Conduct for Players and Player “Code of Conduct” is adopted and part of ICC’s continuing efforts maintain of cricket by (a) effective any participant from conducting themselves improperly on off field-of-play or manner that is contrary to the Spirit Cricket; and (b) robust proportionate procedure pursuant all of conduct can be dealt fairly, with certainty and an expeditious manner. ARTICLE 1 SCOPE AND Player Personnel are automatically bound by and required with all of the Conduct. Accordingly, by their participation (in the case of a Player) assistance Player’s participation case a Personnel in Match, such Players Support Personnel be deemed to have that it is their responsibility to familiarise themselves with the of Conduct, what constitutes an offence'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKGxWcneSFPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}